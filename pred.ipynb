{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    \"\"\"A simple timer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0.\n",
    "\n",
    "    def tic(self):\n",
    "        # using time.time instead of time.clock because time time.clock\n",
    "        # does not normalize for multithreading\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def toc(self, average=True):\n",
    "        self.diff = time.time() - self.start_time\n",
    "        self.total_time += self.diff\n",
    "        self.calls += 1\n",
    "        self.average_time = self.total_time / self.calls\n",
    "        if average:\n",
    "            return self.average_time\n",
    "        else:\n",
    "            return self.diff\n",
    "\n",
    "    def clear(self):\n",
    "        self.total_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product as product\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "class PriorBox(object):\n",
    "    def __init__(self, cfg, image_size=None, phase='train'):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.clip = cfg['clip']\n",
    "        self.image_size = image_size\n",
    "        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n",
    "        self.name = \"s\"\n",
    "\n",
    "    def forward(self):\n",
    "        anchors = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            min_sizes = self.min_sizes[k]\n",
    "            for i, j in product(range(f[0]), range(f[1])):\n",
    "                for min_size in min_sizes:\n",
    "                    s_kx = min_size / self.image_size[1]\n",
    "                    s_ky = min_size / self.image_size[0]\n",
    "                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n",
    "                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n",
    "                    for cy, cx in product(dense_cy, dense_cx):\n",
    "                        anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(anchors).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "def decode_landm(pre, priors, variances):\n",
    "    \"\"\"Decode landm from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): landm predictions for loc layers,\n",
    "            Shape: [num_priors,10]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded landm predictions\n",
    "    \"\"\"\n",
    "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
    "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
    "                        ), dim=1)\n",
    "    return landms\n",
    "\n",
    "\n",
    "\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n",
    "        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n",
    "        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "        top_k: (int) The Maximum number of box preds to consider.\n",
    "    Return:\n",
    "        The indices of the kept boxes with respect to num_priors.\n",
    "    \"\"\"\n",
    "\n",
    "    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mnet = {\n",
    "    'name': 'mobilenet0.25',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 32,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 250,\n",
    "    'decay1': 190,\n",
    "    'decay2': 220,\n",
    "    'image_size': 640,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
    "    'in_channel': 32,\n",
    "    'out_channel': 64\n",
    "}\n",
    "\n",
    "cfg_re50 = {\n",
    "    'name': 'Resnet50',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 24,\n",
    "    'ngpu': 4,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}\n",
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "import torchvision.models._utils as _utils\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_bn_no_relu(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride, leaky=0.1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
    "    )\n",
    "\n",
    "class SSH(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(SSH, self).__init__()\n",
    "        assert out_channel % 4 == 0\n",
    "        leaky = 0\n",
    "        if (out_channel <= 64):\n",
    "            leaky = 0.1\n",
    "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
    "\n",
    "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
    "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv3X3 = self.conv3X3(input)\n",
    "\n",
    "        conv5X5_1 = self.conv5X5_1(input)\n",
    "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
    "\n",
    "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
    "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
    "\n",
    "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self,in_channels_list,out_channels):\n",
    "        super(FPN,self).__init__()\n",
    "        leaky = 0\n",
    "        if (out_channels <= 64):\n",
    "            leaky = 0.1\n",
    "        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n",
    "        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n",
    "\n",
    "        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # names = list(input.keys())\n",
    "        input = list(input.values())\n",
    "\n",
    "        output1 = self.output1(input[0])\n",
    "        output2 = self.output2(input[1])\n",
    "        output3 = self.output3(input[2])\n",
    "\n",
    "        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n",
    "        output2 = output2 + up3\n",
    "        output2 = self.merge2(output2)\n",
    "\n",
    "        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n",
    "        output1 = output1 + up2\n",
    "        output1 = self.merge1(output1)\n",
    "\n",
    "        out = [output1, output2, output3]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MobileNetV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV1, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            conv_bn(3, 8, 2, leaky = 0.1),    # 3\n",
    "            conv_dw(8, 16, 1),   # 7\n",
    "            conv_dw(16, 32, 2),  # 11\n",
    "            conv_dw(32, 32, 1),  # 19\n",
    "            conv_dw(32, 64, 2),  # 27\n",
    "            conv_dw(64, 64, 1),  # 43\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            conv_dw(64, 128, 2),  # 43 + 16 = 59\n",
    "            conv_dw(128, 128, 1), # 59 + 32 = 91\n",
    "            conv_dw(128, 128, 1), # 91 + 32 = 123\n",
    "            conv_dw(128, 128, 1), # 123 + 32 = 155\n",
    "            conv_dw(128, 128, 1), # 155 + 32 = 187\n",
    "            conv_dw(128, 128, 1), # 187 + 32 = 219\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            conv_dw(128, 256, 2), # 219 +3 2 = 241\n",
    "            conv_dw(256, 256, 1), # 241 + 64 = 301\n",
    "        )\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.avg(x)\n",
    "        # x = self.model(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClassHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(ClassHead,self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "        \n",
    "        return out.view(out.shape[0], -1, 2)\n",
    "\n",
    "class BboxHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(BboxHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 4)\n",
    "\n",
    "class LandmarkHead(nn.Module):\n",
    "    def __init__(self,inchannels=512,num_anchors=3):\n",
    "        super(LandmarkHead,self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1x1(x)\n",
    "        out = out.permute(0,2,3,1).contiguous()\n",
    "\n",
    "        return out.view(out.shape[0], -1, 10)\n",
    "\n",
    "class RetinaFace(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "        \"\"\"\n",
    "        :param cfg:  Network related settings.\n",
    "        :param phase: train or test.\n",
    "        \"\"\"\n",
    "        super(RetinaFace,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = None\n",
    "        if cfg['name'] == 'mobilenet0.25':\n",
    "            backbone = MobileNetV1()\n",
    "            if cfg['pretrain']:\n",
    "                checkpoint = torch.load(r\"D:\\Projects\\Pytorch_Retinaface-master\\Pytorch_Retinaface-master\\weights\\mobilenetV1X0.25_pretrain.tar\", map_location=torch.device('cpu'))\n",
    "                from collections import OrderedDict\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in checkpoint['state_dict'].items():\n",
    "                    name = k[7:]  # remove module.\n",
    "                    new_state_dict[name] = v\n",
    "                # load params\n",
    "                backbone.load_state_dict(new_state_dict)\n",
    "        elif cfg['name'] == 'Resnet50':\n",
    "            import torchvision.models as models\n",
    "            backbone = models.resnet50(pretrained=cfg['pretrain'])\n",
    "\n",
    "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
    "        in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            in_channels_stage2 * 2,\n",
    "            in_channels_stage2 * 4,\n",
    "            in_channels_stage2 * 8,\n",
    "        ]\n",
    "        out_channels = cfg['out_channel']\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "        self.ssh2 = SSH(out_channels, out_channels)\n",
    "        self.ssh3 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "    \n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        feature2 = self.ssh2(fpn[1])\n",
    "        feature3 = self.ssh3(fpn[2])\n",
    "        features = [feature1, feature2, feature3]\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from D:\\Projects\\Pytorch_Retinaface-master\\Pytorch_Retinaface-master\\weights\\mobilenet0.25_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:300\n",
      "Finished loading model!\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "confidence_threshold = 0.02\n",
    "nms_threshold = 0.4\n",
    "cfg=cfg_mnet\n",
    "net = RetinaFace(cfg=cfg_mnet, phase = 'test')\n",
    "net = load_model(net, 'D:\\Projects\\Pytorch_Retinaface-master\\Pytorch_Retinaface-master\\weights\\mobilenet0.25_Final.pth', False)\n",
    "cpu = False\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "#print(net)\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\" if cpu else \"cuda\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras_radam import RAdam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aassnaulhq.mp4\n",
      "29.8799\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 805.84 MiB already allocated; 3.99 MiB free; 8.16 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-33fba634c3d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0m_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forward_pass'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0m_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'forward_pass'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0m_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'misc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4a8823d616f2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;31m# FPN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1655\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m     )\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 805.84 MiB already allocated; 3.99 MiB free; 8.16 MiB cached)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "preds = []\n",
    "\n",
    "for x in os.listdir('deepfake-detection-challenge/test_videos'):\n",
    "    # Playing video from file:\n",
    "    print(x)\n",
    "    cap = cv2.VideoCapture(r'G:\\jetjo\\deepfake-detection-challenge\\test_videos\\\\'+x)\n",
    "    print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    arr = []\n",
    "\n",
    "    currentFrame = 0\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        arr.append(frame)\n",
    "    #print(ret)\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    arr = arr[::10]    \n",
    "    real = []\n",
    "    fake = []\n",
    "    cumm = []\n",
    "    for img_raw in arr:\n",
    "    \n",
    "        resize = 1\n",
    "        _t = {'forward_pass': Timer(), 'misc': Timer()}\n",
    "\n",
    "        img = np.float32(img_raw)\n",
    "        if resize != 1:\n",
    "            img = cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n",
    "        im_height, im_width, _ = img.shape\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "        img -= (104, 117, 123)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        scale = scale.to(device)\n",
    "\n",
    "        _t['forward_pass'].tic()\n",
    "        loc, conf, landms = net(img)  # forward pass\n",
    "        _t['forward_pass'].toc()\n",
    "        _t['misc'].tic()\n",
    "        priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "        priors = priorbox.forward()\n",
    "        priors = priors.to(device)\n",
    "        prior_data = priors.data\n",
    "        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        boxes = boxes * scale / resize\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "        landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2]])\n",
    "        scale1 = scale1.to(device)\n",
    "        landms = landms * scale1 / resize\n",
    "        landms = landms.cpu().numpy()\n",
    "\n",
    "        # ignore low scores\n",
    "        inds = np.where(scores > confidence_threshold)[0]\n",
    "        boxes = boxes[inds]\n",
    "        landms = landms[inds]\n",
    "        scores = scores[inds]\n",
    "\n",
    "        # keep top-K before NMS\n",
    "        # order = scores.argsort()[::-1][:args.top_k]\n",
    "        order = scores.argsort()[::-1]\n",
    "        boxes = boxes[order]\n",
    "        landms = landms[order]\n",
    "        scores = scores[order]\n",
    "\n",
    "        # do NMS\n",
    "        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "        keep = py_cpu_nms(dets, nms_threshold)\n",
    "\n",
    "        dets = dets[keep, :]\n",
    "        landms = landms[keep]\n",
    "\n",
    "        # keep top-K faster NMS\n",
    "        # dets = dets[:args.keep_top_k, :]\n",
    "        # landms = landms[:args.keep_top_k, :]\n",
    "\n",
    "        dets = np.concatenate((dets, landms), axis=1)\n",
    "        _t['misc'].toc()\n",
    "\n",
    "        for b in dets:\n",
    "            if b[4] < 0.5:\n",
    "                continue\n",
    "            text = \"{:.4f}\".format(b[4])\n",
    "            b = list(map(int, b))\n",
    "            cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "            img_fin = img_raw[b[1]:b[3],b[0]:b[2]]\n",
    "        #torch.cuda.empty_cache()\n",
    "        model = load_model(r'D:\\Projects\\best_model.hdf5', custom_objects={'RAdam': RAdam})\n",
    "        i = cv2.resize(img_raw, (200,200))\n",
    "        i = np.array(i).reshape((1,200,200, 3))\n",
    "        i = i.astype(float)\n",
    "        i /= 255\n",
    "        prd = model.predict(i)\n",
    "        if prd<.5:\n",
    "            cls = 0\n",
    "            real.append(prd[0][0])\n",
    "        else:\n",
    "            cls = 1\n",
    "            fake.append(prd[0][0])\n",
    "            \n",
    "        cumm.append(cls)\n",
    "    avgsm = 0\n",
    "    frq = max(cumm,key=cumm.count)\n",
    "    #print(frq)\n",
    "    if frq == 0:\n",
    "        for i in real:\n",
    "            avgsm+=i\n",
    "        preds.append(avgsm/len(real))\n",
    "    else:\n",
    "        for i in fake:\n",
    "            avgsm+=i\n",
    "        preds.append(avgsm/len(fake))\n",
    "    #print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    arr = []\n",
    "\n",
    "    currentFrame = 0\n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        arr.append(frame)\n",
    "    #print(ret)\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    arr = arr[::10]    \n",
    "    real = []\n",
    "    fake = []\n",
    "    cumm = []\n",
    "    for img_raw in arr:\n",
    "    \n",
    "        resize = 1\n",
    "        _t = {'forward_pass': Timer(), 'misc': Timer()}\n",
    "\n",
    "        img = np.float32(img_raw)\n",
    "        if resize != 1:\n",
    "            img = cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n",
    "        im_height, im_width, _ = img.shape\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "        img -= (104, 117, 123)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        scale = scale.to(device)\n",
    "\n",
    "        _t['forward_pass'].tic()\n",
    "        loc, conf, landms = net(img)  # forward pass\n",
    "        _t['forward_pass'].toc()\n",
    "        _t['misc'].tic()\n",
    "        priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "        priors = priorbox.forward()\n",
    "        priors = priors.to(device)\n",
    "        prior_data = priors.data\n",
    "        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        boxes = boxes * scale / resize\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "        landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2]])\n",
    "        scale1 = scale1.to(device)\n",
    "        landms = landms * scale1 / resize\n",
    "        landms = landms.cpu().numpy()\n",
    "\n",
    "        # ignore low scores\n",
    "        inds = np.where(scores > confidence_threshold)[0]\n",
    "        boxes = boxes[inds]\n",
    "        landms = landms[inds]\n",
    "        scores = scores[inds]\n",
    "\n",
    "        # keep top-K before NMS\n",
    "        # order = scores.argsort()[::-1][:args.top_k]\n",
    "        order = scores.argsort()[::-1]\n",
    "        boxes = boxes[order]\n",
    "        landms = landms[order]\n",
    "        scores = scores[order]\n",
    "\n",
    "        # do NMS\n",
    "        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "        keep = py_cpu_nms(dets, nms_threshold)\n",
    "\n",
    "        dets = dets[keep, :]\n",
    "        landms = landms[keep]\n",
    "\n",
    "        # keep top-K faster NMS\n",
    "        # dets = dets[:args.keep_top_k, :]\n",
    "        # landms = landms[:args.keep_top_k, :]\n",
    "\n",
    "        dets = np.concatenate((dets, landms), axis=1)\n",
    "        _t['misc'].toc()\n",
    "\n",
    "        for b in dets:\n",
    "            if b[4] < 0.5:\n",
    "                continue\n",
    "            text = \"{:.4f}\".format(b[4])\n",
    "            b = list(map(int, b))\n",
    "            cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "            img_fin = img_raw[b[1]:b[3],b[0]:b[2]]\n",
    "        torch.cuda.empty_cache()\n",
    "        i = cv2.resize(img_raw, (200,200))\n",
    "        i = np.array(i).reshape((1,200,200, 3))\n",
    "        i = i.astype(float)\n",
    "        i /= 255\n",
    "        prd = model.predict(i)\n",
    "        if prd<.5:\n",
    "            cls = 0\n",
    "            real.append(prd[0][0])\n",
    "        else:\n",
    "            cls = 1\n",
    "            fake.append(prd[0][0])\n",
    "            \n",
    "        cumm.append(cls)\n",
    "    avgsm = 0\n",
    "    frq = max(cumm,key=cumm.count)\n",
    "    #print(frq)\n",
    "    if frq == 0:\n",
    "        for i in real:\n",
    "            avgsm+=i\n",
    "        preds.append(avgsm/len(real))\n",
    "    else:\n",
    "        for i in fake:\n",
    "            avgsm+=i\n",
    "        preds.append(avgsm/len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test=pd.read_csv('deepfake-detection-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
